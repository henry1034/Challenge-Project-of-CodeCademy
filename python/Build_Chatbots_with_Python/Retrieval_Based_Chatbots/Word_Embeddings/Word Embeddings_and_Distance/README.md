# [Word Embeddings Are All About Distance](https://www.codecademy.com/paths/build-chatbots-with-python/tracks/retrieval-based-chatbots/modules/nlp-word-embeddings/lessons/word-embeddings/exercises/word-embeddings-distance)

The idea behind word embeddings is a theory known as the distributional hypothesis. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. 

With word embeddings, we map words that exist with the same context to similar places in our vector space.

The numeric values that are assigned to the vector representation of a word are not important in their own right, but gather meaning from how similar or not words are to each other.

The cosine distance between words with similar contexts will be small, and the cosine distance between words that have very different contexts will be large.

The literal values of a wordâ€™s embedding have no actual meaning. We gain value in word embeddings from comparing the different word vectors and seeing how similar or different they are.
